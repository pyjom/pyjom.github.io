---
title: "Database Design using PostgreSQL"
date: 2024-04-14
categories: [Data_Engineering, Database]
tags: [postgreSQL]
---

Final Project: IBM Introduction to Relational Databases
![Alt Text](/assets/lib/img/posts/2024-04-01-db/coffee.png)

In this scenario, I have recently been hired as a Data Engineer by a New York-based coffee shop chain looking to expand nationally by opening several franchise locations. They want to streamline operations and revamp their data infrastructure as part of their expansion process.

My job is to design their relational database systems for improved operational efficiencies and make it easier for their executives to make data-driven decisions.

## Objectives:
1. Design a central database to house all of the data since it resides in several systems.
2. Create a database objects and load them with source data.
3. Load the data into databases using several RDBMS

## Database
- PostgreSQL, IBMDb2
    - Approaches: OLTP 
    - Data Normalization: 2NF

## Datasets
- [IBM Cognos Analytics data sets](https://www.kaggle.com/datasets/ylchang/coffee-shop-sample-data-1113?resource=download)

# Project Proper

### Task 1
- Here, conceptual and logical modeling is done. The sample data is skimmed and entities and attibutes are identified. 
    - Conceptual Modeling describes the entities and relatoonships of the data. The main question here is: 'What is the data structure?'
    - Logical Modeling follows the conceptual modeling and translate it into a structure for RDBMS. The main question here is: 'What is the database model and schema?'

### Task 2
- In task 2, the database is created including the product and sales transaction tables.


### Task 3
- Upon checking the data, it does not conform to second normal form, thus, data normalization is done here. I opt to reach atleast second normal form (2NF). because with second normal form, the concept of constraint especially foreign and referential constraint is used.
    - Data Normalization is the process usually used for OLTP to reduce data redundancy. This promotes data consistency, safety, and easier to redesign in case in need for scalability.

### Task 4
- Here keys and relations are define, using Postgres ERD Tool. It actually quite fun
    - Primary key is the main identifier of a row, this is unique and should not have any null value 
    - Foreign key is the key that connects a table to another table for relationship. The value of foreign key must match the value in primary key.

### Task 5
- In this task, the data is imported, and the data is explored using SQL. 

**_Scenario:_**
_A payroll company has requested a list of employees and the locations at which they work. This list should not include the CEO or CFO who owns the company._
 In this task, I create a view in my PostgreSQL database that returns this information and export the results to a CSV file.

 ```sql
 --- SQL code
SELECT S.staff_id, S.first_name, S.last_name, S.location
FROM staff AS S
WHERE "position" NOT IN ('CEO', 'CFO');
```
>  Fun Fact: This is the only time I fully understand the use case and functionality of View!
>            Views are used to provide understandable interface to access data for users in a quick way.

### Task 6
- Similar problem in the previous task, but in this case materialized view is used

**_Scenario:_** _A marketing consultant requires access to product data in their IBMDb2 for a marketing campaign._
In this task, I create a materialized view in my PostgreSQL database that returns this information and export the results to a CSV file.

```sql
SELECT P.product_name, P.description, PT.product_category
FROM product AS P
JOIN product_type AS PT
ON P.product_type_id = PT.product_type_id;
```

### Task 8
- In this final task, the two csv files are loaded to the IBMDb2. 

![Alt Text](/assets/lib/img/posts/2024-04-01-db/task10.png)

> Check my [github account](https://github.com/pyjom) for images!